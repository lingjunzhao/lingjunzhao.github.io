---
permalink: /
title: "About"
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a 5th-year Ph.D. student at the Computer Science Department of University of Maryland, College Park, where I am advised by Prof. [Hal Daum√© III](http://users.umiacs.umd.edu/~hal3/).

I'm broadly interested in studying problems related to **trustworthiness** (including uncertainty, pragmatics, alignment) in **multimodal** setting, aiming to enhance **human-centered AI**. My research focuses on equipping visual-language models with self-reasoning and self-improvement capabilities to better serve human needs. This includes:\
(i) Correcting hallucinations and communicating uncertainties [[EMNLP 24](https://arxiv.org/abs/2402.16973), [EMNLP 23](https://arxiv.org/abs/2310.15319)]\
(ii) Fostering pragmatic language generation by simulating human behavior [[ACL 23](https://arxiv.org/abs/2301.05149)]\
(iii) Generating faithful free-text explanations [[EMNLP 25](https://arxiv.org/abs/2505.19299)]\
(iv) Visual-language alignment for long context [[ACL 25](https://arxiv.org/abs/2502.15079)]

I'm looking for a research internship in Summer/Spring 2026 with the potential for a return offer, please feel free to reach out for a chat!


# News

* [Nov 2025]  My PhD proposal is: *Toward Faithful and Pragmatic Language Models for Human-Centered AI Agents*
* [Aug 2025]  [New paper](https://arxiv.org/abs/2505.19299) on *A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations* Accepted by EMNLP 2025. [Code](https://github.com/lingjunzhao/PEX_consistency) released.
* [Jun 2025]  Excited to start research internship at Microsoft Semantic Machines!
* [Feb 2025]  [New paper](https://arxiv.org/abs/2502.15079) on *Can Hallucination Correction Improve Video-Language Alignment?* Accepted by ACL 2025.
* [Feb 2024]  New paper on *Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Alternatives*, accepted by EMNLP 2024 (**Oral**). [Project Website](https://lingjunzhao.github.io/HEAR.html)
* [Oct 2023]  New paper on *Hallucination Detection for Grounded Instruction Generation*, accepted by EMNLP 2023. [Project Website](https://lingjunzhao.github.io/hallucination_detection.html)
* [Dec 2022]  New paper on *Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models*. Accepted by ACL 2023, and ICML Theory-of-Mind Workshop 2023 (**Outstanding Paper Award**). [Project Website](https://lingjunzhao.github.io/coop_instruction.html)
