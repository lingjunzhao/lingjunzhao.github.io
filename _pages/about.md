---
permalink: /
title: "About"
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a 5th-year Ph.D. student at the Computer Science Department of University of Maryland, College Park, where I am advised by Prof. [Hal Daum√© III](http://users.umiacs.umd.edu/~hal3/).

I am broadly interested in studying problems related to **trustworthiness** in **multimodal** setting, aiming to enhance **human-centered AI**. My research focuses on equipping visual-language models with self-reasoning and self-improvement capabilities to better serve human needs. This includes:\
(i) Correcting hallucinations and communicating uncertainties [[EMNLP 24](https://arxiv.org/abs/2402.16973), [EMNLP 23](https://arxiv.org/abs/2310.15319)]\
(ii) Fostering pragmatic understanding by simulating human behavior [[ACL 23](https://arxiv.org/abs/2301.05149)]\
(iii) Generating faithful explanations [Paper coming soon]\
(iv) Visual-language alignment for long context [[ACL 25](https://arxiv.org/abs/2502.15079)]


# News

* [Jun 2025]  Excited to start research internship at Microsoft Semantic Machines!
* [Feb 2025]  [New paper](https://arxiv.org/abs/2502.15079) on *Can Hallucination Correction Improve Video-Language Alignment?* Accepted by ACL 2025.
* [Feb 2024]  New paper on *Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Alternatives*, accepted by EMNLP 2024 (**Oral**). [Project Website](https://lingjunzhao.github.io/HEAR.html)
* [Oct 2023]  New paper on *Hallucination Detection for Grounded Instruction Generation*, accepted by EMNLP 2023. [Project Website](https://lingjunzhao.github.io/hallucination_detection.html)
* [Dec 2022]  New paper on *Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models*. Accepted by ACL 2023, and ICML Theory-of-Mind Workshop 2023 (**Outstanding Paper Award**). [Project Website](https://lingjunzhao.github.io/coop_instruction.html)
