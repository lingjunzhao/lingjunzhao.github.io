---
permalink: /
title: "About"
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a 5th-year Ph.D. student at the Computer Science Department of University of Maryland, College Park, where I am advised by Prof. [Hal Daum√© III](http://users.umiacs.umd.edu/~hal3/). 

My research focuses on **making (visual) language models more trustworthy**--addressing uncertainty, pragmatics, and alignment--by equipping them with self-reasoning and self-improvement capabilities. Ultimately, I aim to advance human-centered AI.
This includes:\
(i) Correcting hallucinations and communicating uncertainties [[EMNLP 24](https://arxiv.org/abs/2402.16973), [EMNLP 23](https://arxiv.org/abs/2310.15319)]\
(ii) Fostering pragmatic language generation by simulating human behavior [[ACL 23](https://arxiv.org/abs/2301.05149)]\
(iii) Generating faithful free-text explanations [[EMNLP 25](https://arxiv.org/abs/2505.19299)]\
(iv) Improving visual-language alignment for long context understanding [[ACL 25](https://arxiv.org/abs/2502.15079)]

Previously, I interned at Microsoft Research, working on personalization, and at Honda Research Institute, focusing on visual language models. I also worked as a staff scientist at BBN Technologies on cross-lingual information retrieval.

I'm currently seeking a research internship in Summer/Spring 2026 with the potential for return, please feel free to reach out for a chat!


# News

* [Nov 2025]  My PhD proposal is: *Toward Faithful and Pragmatic Language Models for Human-Centered AI Agents*
* [Aug 2025]  [New paper](https://arxiv.org/abs/2505.19299) on *A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations* Accepted by EMNLP 2025. [Code](https://github.com/lingjunzhao/PEX_consistency) released.
* [Jun 2025]  Excited to start research internship at Microsoft Semantic Machines!
* [Feb 2025]  [New paper](https://arxiv.org/abs/2502.15079) on *Can Hallucination Correction Improve Video-Language Alignment?* Accepted by ACL 2025.
* [Feb 2024]  New paper on *Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Alternatives*, accepted by EMNLP 2024 (**Oral**). [Project Website](https://lingjunzhao.github.io/HEAR.html)
* [Oct 2023]  New paper on *Hallucination Detection for Grounded Instruction Generation*, accepted by EMNLP 2023. [Project Website](https://lingjunzhao.github.io/hallucination_detection.html)
* [Dec 2022]  New paper on *Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models*. Accepted by ACL 2023, and ICML Theory-of-Mind Workshop 2023 (**Outstanding Paper Award**). [Project Website](https://lingjunzhao.github.io/coop_instruction.html)
